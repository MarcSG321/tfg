import os
import re
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2
import easyocr
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Convolution2D,Dropout, BatchNormalization

import os
import re
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

def add_padding(image_array, target_width=500, target_height=250):
    # Convertir la matriz NumPy en una imagen PIL
    image = Image.fromarray(image_array.astype('uint8'), 'L')  # Convertir a escala de grises
    
    # Obtener las dimensiones originales de la imagen
    width, height = image.size
    
    # Calcular la cantidad de padding necesario en cada lado
    horizontal_padding = (target_width - width) // 2
    vertical_padding = (target_height - height) // 2
    
    # Crear una nueva imagen con el tamaño deseado y color de fondo blanco
    padded_image = Image.new('L', (target_width, target_height), 128)  # Modo 'L' para escala de grises
    
    # Pegar la imagen original en el centro de la nueva imagen
    padded_image.paste(image, (horizontal_padding, vertical_padding))
    
    # Convertir la imagen PIL de vuelta a una matriz NumPy
    padded_image_array = np.array(padded_image)
    
    return padded_image_array



ruta_train = "D:/marcs/CIENCIA DE DATOS/CUARTO/TFG/ICDAR2017/TRAIN"

ruta_val = "D:/marcs/CIENCIA DE DATOS/CUARTO/TFG/ICDAR2017/VAL"

ruta_test = "D:/marcs/CIENCIA DE DATOS/CUARTO/TFG/ICDAR2017/TEST"


carpetas = os.listdir(ruta_test)


## TANTO EN TRAIN TEST Y VAL TIENEN LAS CARPETAS CON EL MISMO NOMBRE


# Expresión regular para encontrar carpetas con nombres que comienzan con 'M' seguido de números

patron = re.compile(r'^B_BOXES_\d+$')

# Me quedo con las carpetas que cumplen el patrón 

carpetas_train=[carpeta for carpeta in carpetas if patron.match(carpeta)] ## LAS CARPETAS SON LAS MISMAS PARA LOS TRES CONJUNTOS




imagenes_train=[]
etiquetas_train=[]
etiqueta=0
img=0
for carpeta in carpetas_train:
    ruta_carpeta=f"{ruta_train}/{carpeta}"
    images=os.listdir(ruta_carpeta)
    for image in images:
        caja=0
        foto=Image.open(f"{ruta_train}/{carpeta}/{image}").convert('L')
        h,w=np.array(foto).shape
        if not(w<30 or w>500):
            if not(h<30 or h>250):
                imagenes_train.append(add_padding(np.array(foto)))
                etiquetas_train.append(etiqueta)
    
    etiqueta+=1


imagenes_val=[]
etiquetas_val=[]
etiqueta=0
img=0
for carpeta in carpetas_train:
    ruta_carpeta=f"{ruta_val}/{carpeta}"
    images=os.listdir(ruta_carpeta)
    for image in images:
        caja=0
        foto=Image.open(f"{ruta_val}/{carpeta}/{image}").convert('L')
        h,w=np.array(foto).shape
        if not(w<30 or w>500):
            if not(h<30 or h>250):
            imagenes_val.append(add_padding(np.array(foto)))
            etiquetas_val.append(etiqueta)
    
    etiqueta+=1


imagenes_test=[]
etiquetas_test=[]
etiqueta=0
img=0
for carpeta in carpetas_train:
    ruta_carpeta=f"{ruta_test}/{carpeta}"
    images=os.listdir(ruta_carpeta)
    for image in images:
        caja=0
        foto=Image.open(f"{ruta_test}/{carpeta}/{image}").convert('L')
        h,w=np.array(foto).shape
        if not(w<30 or w>500):
            if not(h<30 or h>250):
                imagenes_test.append(add_padding(np.array(foto)))
                etiquetas_test.append(etiqueta)
    
    etiqueta+=1


## MODELOS

X_train=imagenes_train.copy()
Y_train=etiquetas_train.copy()


X_val=imagenes_val.copy()
Y_val=etiquetas_val.copy()


X_test=imagenes_train.copy()
Y_test=etiquetas_train.copy()


# SE INCLUYE UNA CONFIGURACIÓN DE LOS MODELOS USADOS, YA QUE ES MUY FÁCIL ADAPTARLO.

  model = Sequential()

# Agregar capas convolucionales y de pooling
model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(250, 500, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Conv2D(64, (5, 5), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(30, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#------------------------------------------------
ONE-HOT


Y_train=keras.utils.to_categorical(Y_train,num_classes=30)
Y_val=keras.utils.to_categorical(Y_val,num_classes=30)


early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=10,
    verbose=1,
    restore_best_weights=True
)



def calcular_top3_accuracy(predicciones, etiquetas_verdaderas):
    # Obtén las tres clases con las probabilidades más altas para cada muestra
    top3_clases = argsort(predicciones, axis=1)[:, -3:]

    # Comprueba si la clase verdadera está entre las tres clases con las probabilidades más altas
    top3_correctas = sum(1 for i in range(len(etiquetas_verdaderas)) if etiquetas_verdaderas[i] in top3_clases[i])

    # Calcula el top-3 accuracy
    top3_accuracy = top3_correctas / len(etiquetas_verdaderas)

    return top3_accuracy




X_test=[]
for i in range(30):
  patches=[]
  for j in range(len(Y_test)):
    if y_test[j]==i:
      patches.append(Y_test[j])
  X_test.append(patches)


predicciones=[]
pred5=[]
predics=[]
h=0
for imagen in X_test:
  pred=[0]*30
  for patch in imagen:
      prediccion=model.predict(np.reshape(patch, (-1, 250, 500, 1)))
      for i in range(len(pred)):
        pred[i]+=list(prediccion[0])[i]/len(imagen)
      print(len(x_test)-h)
      h+=1
  predic=np.argmax(np.array(pred))
  tres_maximos = np.argpartition(pred, -3)[-3:]
  indices_maximos = tres_maximos[tres_maximos.argsort()]
  predicciones.append(predic)
  pred5.append(indices_maximos)



aciertos=0
for i in range(len(y_TEST)):
  if y_TEST[i]==predicciones[i]:
    aciertos+=1
print(aciertos/30)

ac5=0
for i in range(len(y_TEST)):
  if y_TEST[i] in pred5[i]:
    ac5+=1
print(ac5/30)


















